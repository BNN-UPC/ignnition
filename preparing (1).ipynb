{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import transformerblock as tran\n",
    "import GCN\n",
    "import os\n",
    "import matplotlib.pyplot as plot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sembla que els .h m'estan donant massa problemes, si els csv surten bé i es poden importar correctament seria l'hostia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dades = pd.read_hdf('datasets/metr-la.h5')\n",
    "# freq = 0\n",
    "# dset = dades['773869']\n",
    "\n",
    "\n",
    "second = pd.read_csv('datasets/PEMS-BAY.csv')\n",
    "dset2 = second['400001']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = pd.read_pickle('datasets/adj_mx_PEMS-BAY.pkl')\n",
    "adj_matrx = matrix\n",
    "\n",
    "\"\"\"\n",
    "def load_graph_data(pkl_filename):\n",
    "    sensor_ids, sensor_id_to_ind, adj_mx = load_pickle(pkl_filename)\n",
    "    return sensor_ids, sensor_id_to_ind, adj_mx\n",
    "\"\"\"\n",
    "\n",
    "adj_mtx = sp.coo_matrix(adj_matrx[2])\n",
    "# grafo = nx.Graph(adj_mtx)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En els papers diuen que utilitzen un model per fer la espacial que es basa en facer una regularització laplaciana en les funcions de perdues\n",
    "\n",
    "segons el paper: L = L0 + sigmaLreg\n",
    "\n",
    " Lreg =sumatori i,j Aij ||f(Xi) − f(Xj )^2|| = f(X)^T ∆f(X).\n",
    "\n",
    "L0 es la perdua supervisada\n",
    "\n",
    "El Xout de la espacial ha de ser\n",
    "Xout = σ(D˜ ^-1/2 A˜ D˜ ^−1/2 Xin W) \n",
    "la sigma aes una ReLU(·)\n",
    "\n",
    "A~ = A + In on  a es la matriu d'adjecencia i l'altre la identitat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Normalitzar adjecencia, funcion treta d'un paper\n",
    "\n",
    "def degree_mat(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    rowsum = np.array(adj.sum(1)) # aqui seria d'on sortirien els valors de la diagonal \n",
    "    # d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    # d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    # d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    # return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
    "    return rowsum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ref_adj(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    rowsum = np.array(adj.sum(1)) # aqui seria d'on sortirien els valors de la diagonal \n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_to_tuple(sparse_mx):\n",
    "    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n",
    "    def to_tuple(mx):\n",
    "        if not sp.isspmatrix_coo(mx):\n",
    "            mx = mx.tocoo()\n",
    "        coords = np.vstack((mx.row, mx.col)).transpose()\n",
    "        values = mx.data\n",
    "        shape = mx.shape\n",
    "        return coords, values, shape\n",
    "\n",
    "    if isinstance(sparse_mx, list):\n",
    "        for i in range(len(sparse_mx)):\n",
    "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
    "    else:\n",
    "        sparse_mx = to_tuple(sparse_mx)\n",
    "\n",
    "    return sparse_mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_features(features):\n",
    "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
    "    rowsum = np.array(features.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    features = r_mat_inv.dot(features)\n",
    "    return sparse_to_tuple(features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per definir el graf del sistema ho farem de la seguent manera\n",
    "\n",
    "Vertex hidden value -> Valor Q dels sensors Edge value -> VAlor de l'adjecencia de la matriu R\n",
    "\n",
    "Funcions del message passing: Update -> suma del valor anterior Agregació -> convolució del graf W -> el valor de la funció Xout\n",
    "\n",
    "x' = Wx'i + W Sumatori(xj / sqrt(di*dj)) per j pertanyent a N(i)U(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_mtx_csr = adj_mtx.tocsr()\n",
    "adj_csr_rdx = adj_mtx_csr[00:10,00:10]\n",
    "D = degree_mat(adj_csr_rdx) # matriu que ens mostra el nivell de conexió del sensor\n",
    "A = ref_adj(adj_csr_rdx) # matriu d'adjacencies ajustada i refinada\n",
    "A_tp = sparse_to_tuple(A)\n",
    "Ds = sp.csr_matrix(D)\n",
    "D_tp = sparse_to_tuple(Ds)\n",
    "#com que es la pemsbay es el dataset second\n",
    "hidden_value_all= second #matriu en forma de fila 0 index nom dels sensors i cap avall en les columnes el valor q de cada sensor cada 5 minuts\n",
    "\n",
    "#####seria una bona idea treballar amb 10 sensors per ara, k es més visual\n",
    "\n",
    "hidden_value = hidden_value_all.iloc[:1, 1:11]\n",
    "\n",
    "hidden_A  = hidden_value.to_numpy()\n",
    "hidden_valuem = sp.csr_matrix(hidden_value)\n",
    "hidden_value = preprocess_features(hidden_valuem)\n",
    "\n",
    "#en la primera instancia del temps els valors q de tots els sensors\n",
    "# Hs = sp.csr_matrix(hidden_value)\n",
    "# H_tp = sparse_to_tuple(Hs)\n",
    "# graph = nx.from_scipy_sparse_matrix(A.tocsc())\n",
    "# print(nx.edges(graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_tensors(mtx):\n",
    "    idx, idy, val = sp.find(mtx)\n",
    "    full_indices = tf.stack([idx, idy], axis=1)\n",
    "    tamany = mtx.shape\n",
    "    full_indices=tf.cast(full_indices,dtype=tf.int64)\n",
    "    sparse = tf.sparse.SparseTensor(full_indices,val,tamany)\n",
    "    sparse=tf.sparse.reorder(sparse)\n",
    "    # dense = tf.sparse.to_dense(sparse)\n",
    "    return(sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_passing(A,D,hidden):\n",
    "    A64 = tf.cast(A,dtype=tf.float64)\n",
    "    A_Hidd = tf.stack([A64,hidden])\n",
    "    D_layyer = layers.Dense()\n",
    "    output = D_layyer(A_Hidd)\n",
    "\n",
    "    #aggregation of messages\n",
    "    aggr = tf.math.segment_sum(output,D)\n",
    "    return(aggr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[71.4, 67.8, 125.39743, 67.4, 54.39285, 66.6, 43.50115, 68.0, 66.8, 69.0]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def one_hop(F, A):\n",
    "    Fp =[]\n",
    "    for i in range(A[0].size):\n",
    "        aux = F[i]*A[i][0]\n",
    "        Fp.append(np.sum(aux))\n",
    "    return(Fp)\n",
    "\n",
    "A_d = A.todense()\n",
    "passe1 = one_hop(hidden_A[0],A_d)\n",
    "one_hop(passe1,A_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mask_A(mat):\n",
    "    A_mask = mat\n",
    "    for i in range(mat.shape[0]):\n",
    "        for j in range(mat[i].size):\n",
    "            if mat[i,j]> 0:\n",
    "                A_mask[i,j]=1\n",
    "    return(A_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ffn(hidden_units, dropout_rate, name=None):\n",
    "    fnn_layers = []\n",
    "    fnn_layers.append(layers.BatchNormalization())\n",
    "\n",
    "    for units in hidden_units:\n",
    "        fnn_layers.append(layers.Dense(units, activation=tf.nn.relu))\n",
    "        fnn_layers.append(layers.Dropout(dropout_rate))\n",
    "        \n",
    "    fnn_layers.append(layers.Dense(units, activation=tf.nn.relu))\n",
    "    return keras.Sequential(fnn_layers, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#quan crido la gcn els inputs ahn d'estar en una sola entrada de manera\n",
    "# node_features les features de cada node [num_nodes, num_features]\n",
    "# edges matriu adjacencia enmascarada per que siguin 1 i 0 i es marqui adjacencia pura, ha de ser sparse matrix\n",
    "# la aquesta ha de ser shape [2, num_edges]\n",
    "# edge wheights nunmpy array amb els pesos del edges [num_edges]\n",
    "A_mask = Mask_A(A.tocsr().toarray()) #aquesta funcio enmascara la adjecencia normalizada en si o no, el pesos estan guardats en altres matrius\n",
    "A_mask = sp.csr_matrix(A_mask)\n",
    "keys_a =[]\n",
    "\n",
    "for i in range(A_mask.toarray()[0].size):\n",
    "    for j in range(A_mask[i].size):\n",
    "        keys_a.append(i)\n",
    "a_keys = A_mask.indices\n",
    "keys_a = np.array(keys_a)\n",
    "edges = np.array([a_keys,keys_a])\n",
    "\n",
    "hidden_B = tf.cast(hidden_A,dtype=tf.float32)\n",
    "hidden_C = tf.reshape(hidden_B, [10,1])\n",
    "a_p = A.tocsr()\n",
    "\n",
    "ffn1 = create_ffn([1], 0.2, name=\"preprocess\")\n",
    "node_representations = ffn1(hidden_C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class convolutions_passing(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        graph_info,\n",
    "        hidden_units,\n",
    "        aggregation_type=\"sum\",\n",
    "        combination_type=\"gru\",\n",
    "        dropout_rate=0.2,\n",
    "        normalize=False,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        # Unpack graph_info to three elements: node_features, edges, and edge_weight.\n",
    "        edges, edge_weights = graph_info\n",
    "        self.edges = edges\n",
    "        self.edge_weights = edge_weights\n",
    "        # Set edge_weights to ones if not provided.\n",
    "        if self.edge_weights is None:\n",
    "            self.edge_weights = tf.ones(shape=edges.shape[1])\n",
    "        # Scale edge_weights to sum to 1.\n",
    "        self.edge_weights = self.edge_weights / tf.math.reduce_sum(self.edge_weights)\n",
    "\n",
    "        # Create a process layer.\n",
    "        self.preprocess = create_ffn(hidden_units, dropout_rate, name=\"preprocess\")\n",
    "        # Create the first GraphConv layer.\n",
    "        self.conv1 = GCN.GraphConvLayer(\n",
    "            hidden_units,\n",
    "            dropout_rate,\n",
    "            aggregation_type,\n",
    "            combination_type,\n",
    "            normalize,\n",
    "            name=\"graph_conv1\",\n",
    "        )\n",
    "        # Create the second GraphConv layer.\n",
    "        self.conv2 = GCN.GraphConvLayer(\n",
    "            hidden_units,\n",
    "            dropout_rate,\n",
    "            aggregation_type,\n",
    "            combination_type,\n",
    "            normalize,\n",
    "            name=\"graph_conv2\",\n",
    "        )\n",
    "        # Create a postprocess layer.\n",
    "        self.postprocess = create_ffn(hidden_units, dropout_rate, name=\"postprocess\")\n",
    "        # Create a compute logits layer.\n",
    "        # self.compute_logits = layers.Dense(units=num_classes, name=\"logits\")\n",
    "\n",
    "    def call(self, input_node_features):\n",
    "        # Preprocess the node_features to produce node representation\n",
    "        x = self.preprocess(input_node_features)\n",
    "        # Apply the first graph conv layer.\n",
    "        x1 = self.conv1((x, self.edges, self.edge_weights))\n",
    "        # Skip connection.\n",
    "        x = x1 + x\n",
    "        # Apply the second graph conv layer.\n",
    "        x2 = self.conv2((x, self.edges, self.edge_weights))\n",
    "        # Skip connection.\n",
    "        x = x2 + x\n",
    "        # Postprocess node embedding.\n",
    "        x = self.postprocess(x)\n",
    "        # Fetch node embeddings for the input node_indices.\n",
    "        #node_embeddings = tf.gather(x, input_node_indices)\n",
    "        # Compute logits\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2694)\n"
     ]
    }
   ],
   "source": [
    "#ja tenim el model construit, ara es qüestió de entrenar-lo i mirar de que estigui apunt\n",
    "hidden_value = hidden_value_all.iloc[:1, 1:]\n",
    "hidden_D  = hidden_value.to_numpy()\n",
    "hidden_E = tf.cast(hidden_D,dtype=tf.float32)\n",
    "hidden_F = tf.reshape(hidden_E, [hidden_D.size,1])\n",
    "node_rep=ffn1(hidden_F)\n",
    "\n",
    "adj_mtx_csr = adj_mtx.tocsr()\n",
    "A = ref_adj(adj_mtx_csr)\n",
    "\n",
    "A_mask = Mask_A(A.tocsr().toarray()) #aquesta funcio enmascara la adjecencia normalizada en si o no, el pesos estan guardats en altres matrius\n",
    "A_mask = sp.csr_matrix(A_mask)\n",
    "keys_a =[]\n",
    "for i in range(A_mask.toarray()[0].size):\n",
    "    for j in range(A_mask[i].size):\n",
    "        keys_a.append(i)\n",
    "a_keys = A_mask.indices\n",
    "keys_a = np.array(keys_a)\n",
    "edges = np.array([a_keys,keys_a])\n",
    "preprocess = create_ffn([1], 0.2, name=\"preprocess\")\n",
    "a_p = A.tocsr()\n",
    "tf.print(edges.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_value_next = hidden_value_all.iloc[1:2, 1:]\n",
    "hidden_D_next  = hidden_value_next.to_numpy()\n",
    "hidden_E_next = tf.cast(hidden_D_next,dtype=tf.float32)\n",
    "hidden_F_next = tf.reshape(hidden_E_next, [hidden_D_next.size,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_F.fit(\n",
    "#     x=hidden_F,\n",
    "#     y=hidden_F_next,\n",
    "#     epochs = 100,\n",
    "#     batch_size=325,\n",
    "#     use_multiprocessing=True\n",
    "# )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "manera de fer que tot estigui com hauria d'estar, s'ha de fer un reshape general del dataset segur \n",
    "\n",
    "donatt que el dataset original es segur [x,325] on x son els timesteps que tenim, es podrien tindre de manera senzilla amb una eina de tall o demanant el size [,1].size\n",
    "\n",
    "fent la transposició ho fa tot molt més facil per operar, la matriu d'adjacencia s'hauria de modificar per si aca, crec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_value_sans_names = hidden_value_all.iloc[:, 1:]\n",
    "\n",
    "hidden_D_sans  = hidden_value_sans_names.to_numpy()\n",
    "hidden_E_sans = tf.cast(hidden_D_sans,dtype=tf.float32)\n",
    "\n",
    "shapes = hidden_E_sans.shape\n",
    "\n",
    "noves_shapes = tf.reshape(hidden_E_sans, [shapes[1],shapes[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor_layer(name=\"Predictor\"):\n",
    "    predictor_layers =[]\n",
    "    predictor_layers.append(keras.layers.Dense(64,activation='relu'))\n",
    "    predictor_layers.append(keras.layers.Dense(32,activation='relu'))\n",
    "    predictor_layers.append(keras.layers.Dense(1))\n",
    "\n",
    "    return keras.Sequential(predictor_layers, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mlp_layer(name=\"mlp_placeholder\"):\n",
    "#     mlp_layers =[]\n",
    "#     mlp_layers.append(keras.layers.Dense(,activation='sigmoid'))\n",
    "#     mlp.layers.append(keras.layers.Dense(,activation='sigmoid'))\n",
    "#     mlp.layers.append(keras.layers.Dense(,activation='sigmoid'))\n",
    "\n",
    "#     return keras.Sequential(mlp_layers, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STGNN(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        graph_info, #matriu d'adjecencia del graph\n",
    "        hidden_units, #quantes features estarem tractant per timestep i sensor\n",
    "        aggregation_type=\"sum\",\n",
    "        combination_type=\"gru\",\n",
    "        dropout_rate=0.2,\n",
    "        normalize=False,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        # Unpack graph_info to three elements: node_features, edges, and edge_weight.\n",
    "        edges, edge_weights = graph_info\n",
    "        self.edges = edges\n",
    "        self.edge_weights = edge_weights\n",
    "        # Set edge_weights to ones if not provided.\n",
    "        if self.edge_weights is None:\n",
    "            self.edge_weights = tf.ones(shape=edges.shape[1])\n",
    "        # Scale edge_weights to sum to 1.\n",
    "        self.edge_weights = self.edge_weights / tf.math.reduce_sum(self.edge_weights)\n",
    "        self.aggregation_type = aggregation_type\n",
    "        self.combination_type = combination_type\n",
    "        self.normalize = normalize\n",
    "        self.hidden_units = hidden_units\n",
    "\n",
    "\n",
    "        # Create a process layer.\n",
    "        self.preprocess = create_ffn(hidden_units, dropout_rate, name=\"preprocess\")\n",
    "        # Create the first GraphConv layer.\n",
    "        self.conv1 = GCN.GraphConvLayer(\n",
    "            hidden_units,\n",
    "            dropout_rate,\n",
    "            aggregation_type,\n",
    "            combination_type,\n",
    "            normalize,\n",
    "            name=\"graph_conv1\",\n",
    "        )\n",
    "        # Create a postprocess layer.\n",
    "        # self.postprocess = create_ffn(hidden_units, dropout_rate, name=\"postprocess\")\n",
    "        # Create a compute logits layer.\n",
    "        # self.compute_logits = layers.Dense(units=num_classes, name=\"logits\")\n",
    "\n",
    "        self.grucell = tf.keras.layers.GRUCell(64,name=\"Gru_temporal\")\n",
    "\n",
    "        # self.MLP = mlp_layer()\n",
    "\n",
    "        #self.NovaGRU = tf.keras.layers.GRU(64,name=\"Gru_final\")\n",
    "        \n",
    "        self.tbloc = tran.TransformerBlock(\n",
    "            embed_dim=64, \n",
    "            num_heads=4, \n",
    "            ff_dim=64\n",
    "        )\n",
    "\n",
    "        \n",
    "        self.predictor = predictor_layer()\n",
    "        # la prediction layer es una multi feed-forward network per predir veloocitat transit\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "    def call(self, input_node_features_all):\n",
    "        # Fem una petita modificació, ara entra tot el dataset de cop (52116, 325)\n",
    "        # pero igualment els primers tractaments son de timestep a timestep\n",
    "        # per tant primer s'itera per alla i finalment s'aglomera tot i es fa el predictor\n",
    "\n",
    "        # Fare un apetita consideracio, nomes plantejare un batch de [1,12,325]\n",
    "        aux = input_node_features_all[0]\n",
    "        # print(old_x.shape)\n",
    "        zerox=aux[0]\n",
    "        # tf.print(\"primera valua per a mes tard old_x: \", old_x.shape)\n",
    "        old_x = tf.expand_dims(zerox,axis=1)\n",
    "        # tf.print(\"reshape de old_x fet\", old_x.shape)\n",
    "        old_x = self.preprocess(old_x)\n",
    "        # tf.print(\"prepo old_x fet_:\", old_x.shape)\n",
    "        x_full= tf.expand_dims(old_x,axis=1)\n",
    "        # tf.print(\"Hem expandit dimensioins i entrarem al loop\")\n",
    "        iter_count=0\n",
    "        # tf.print(x_full.shape)\n",
    "        #mirar si es correcte la manera en que accedim a les dades?\n",
    "        # tf.print(input_node_features_all.shape)\n",
    "        for input_node_features in aux[1:] :\n",
    "            \n",
    "            tf.autograph.experimental.set_loop_options(\n",
    "            shape_invariants=[(x_full, tf.TensorShape([x_full.shape[0],None,x_full.shape[2]]))]\n",
    "            )\n",
    "            # Preprocess the node_features to produce node representation\n",
    "            x= tf.expand_dims(input_node_features,axis=1)\n",
    "            # x = tf.reshape(input_node_features,[input_node_features.shape[1],input_node_features.shape[0]])\n",
    "            # tf.print(x.shape)\n",
    "            x = self.preprocess(x)\n",
    "            # Apply the first graph conv layer.\n",
    "            x1 = self.conv1((x, self.edges, self.edge_weights))\n",
    "            # tf.print(\"Despres de la convo: \", x1.shape)\n",
    "            # Skip connection.\n",
    "            x = x1 + x\n",
    "            # Apply the second graph conv layer.\n",
    "            # Postprocess node embedding.\n",
    "            x, x2 = self.grucell(x,old_x) \n",
    "            # tf.print(\"post grucell: \", x2.shape)\n",
    "            old_x = x2\n",
    "            # tf.print(\"x_full shape:\",x_full.shape)\n",
    "            x_full = tf.concat([x_full,tf.expand_dims(old_x,axis=1)],1)\n",
    "            # tf.print(\"x_full despres del concat x_full:\", x_full.shape)\n",
    "            iter_count +=1\n",
    "        \n",
    "\n",
    "        \n",
    "        x_full = tf.convert_to_tensor(x_full,tf.float32)\n",
    "        # tf.print(\"x_full despres d'un bucle per si un cas vull veure que passa\",x_full.shape)\n",
    "        # tf.print(x_full.shape)\n",
    "        #res = self.NovaGRU(x_full)\n",
    "        # tf.print(x_full.shape) #325, 12, 64\n",
    "        # sortiran bastant més valors del transformer, es pot provar amb i o sense transformer\n",
    "\n",
    "        res = self.tbloc(x_full,x_full) #hauria de treure 325, 64\n",
    "        # tf.print(\"sortida del transformer: \", res.shape)\n",
    "        # tf.print(res.shape)\n",
    "\n",
    "        x = self.predictor(res) # hauria de treure 325, 12 SEWMPRE una sortida\n",
    "        \n",
    "        x=tf.squeeze(x)\n",
    "        \n",
    "        x= tf.transpose(x)\n",
    "        return x\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Full = STGNN(    \n",
    "    graph_info=(edges, a_p.data),\n",
    "    hidden_units=[64],\n",
    "    dropout_rate=0.2,\n",
    "    name=\"Spatio_temporal_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_M_sans  = hidden_value_sans_names.iloc[:5,:]\n",
    "\n",
    "alll = tf.cast(hidden_value_sans_names,dtype=tf.float32)\n",
    "\n",
    "mitja_Set = tf.cast(hidden_M_sans,dtype=tf.float32)\n",
    "\n",
    "next_M_sans  = hidden_value_sans_names.iloc[5:10,:]\n",
    "\n",
    "\n",
    "\n",
    "nexting_set = tf.cast(next_M_sans,dtype=tf.float32)\n",
    "\n",
    "mitja_SET = tf.reshape(mitja_Set,[mitja_Set.shape[1],mitja_Set.shape[0]])\n",
    "nexting_SET = tf.reshape(nexting_set,[nexting_set.shape[1],nexting_set.shape[0]])\n",
    "\n",
    "\n",
    " #enviar en troços petits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52092, 325, 12)\n"
     ]
    }
   ],
   "source": [
    "alll_t  = tf.transpose(alll)\n",
    "\n",
    "win_size=12\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(win_size, alll.shape[0]-win_size):\n",
    "    x_train.append(alll[i-win_size:i, :])\n",
    "    y_train.append(alll[i:win_size+i, :])\n",
    "\n",
    "x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "train_y=np.reshape(y_train,(y_train.shape[0],y_train.shape[2],y_train.shape[1]))\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.35053957  0.3488285   0.35009933 ...  0.34912318  0.35017252\n",
      "   0.34885836]\n",
      " [-0.14089811 -0.13997126 -0.14109683 ... -0.13606536 -0.13291311\n",
      "  -0.1341443 ]\n",
      " [-0.8953564  -0.8902283  -0.8926969  ... -0.89045537 -0.8917784\n",
      "  -0.8905465 ]\n",
      " ...\n",
      " [-1.419546   -1.3566755  -1.4073583  ... -1.3753352  -1.414859\n",
      "  -1.3601422 ]\n",
      " [-1.3992493  -1.3356683  -1.3803463  ... -1.3520825  -1.3913398\n",
      "  -1.3382882 ]\n",
      " [-1.3744205  -1.3088739  -1.3585227  ... -1.3279381  -1.3680899\n",
      "  -1.3122844 ]], shape=(12, 325), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(model_Full(x_train[:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Spatio_temporal_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "preprocess (Sequential)      (325, 64)                 4292      \n",
      "_________________________________________________________________\n",
      "graph_conv1 (GraphConvLayer) multiple                  8832      \n",
      "_________________________________________________________________\n",
      "Gru_temporal (GRUCell)       multiple                  24960     \n",
      "_________________________________________________________________\n",
      "transformer_block (Transform multiple                  74944     \n",
      "_________________________________________________________________\n",
      "Predictor (Sequential)       (325, 12, 1)              6273      \n",
      "=================================================================\n",
      "Total params: 119,301\n",
      "Trainable params: 119,043\n",
      "Non-trainable params: 258\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# model_Full.build(input_shape=[5,325])\n",
    "print(model_Full.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52092, 325, 12)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "alll_t  = tf.transpose(alll)\n",
    "\n",
    "win_size=12\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(win_size, alll.shape[0]-win_size):\n",
    "    x_train.append(alll[i-win_size:i, :])\n",
    "    y_train.append(alll[i:win_size+i, :])\n",
    "\n",
    "x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "train_y=np.reshape(y_train,(y_train.shape[0],y_train.shape[2],y_train.shape[1]))\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4092, 12, 325)\n"
     ]
    }
   ],
   "source": [
    "x_test=x_train[48000:]\n",
    "y_test=y_train[48000:]\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52092, 12, 325)\n"
     ]
    }
   ],
   "source": [
    "# Prepare the test data\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "for i in range(win_size, alll.shape[0]-win_size):\n",
    "    x_test.append(alll[i-win_size:i, :])\n",
    "    y_test.append(alll[i:win_size+i, :])\n",
    "\n",
    "x_test, y_test = np.array(x_test), np.array(y_test)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4092, 325, 12)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_test=x_test[48000:]\n",
    "y_test=y_test[48000:]\n",
    "\n",
    "test_x=np.reshape(x_test,(x_test.shape[0],x_test.shape[2],x_test.shape[1]))\n",
    "test_y=np.reshape(y_test,(y_test.shape[0],y_test.shape[2],y_test.shape[1]))\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5000/5000 [==============================] - 500s 99ms/step - loss: 3.9443 - mae: 3.9443 - mape: 752515.8125 - rmse: 8.0352 - val_loss: 4.9219 - val_mae: 4.9219 - val_mape: 12.2398 - val_rmse: 8.0758\n",
      "\n",
      "Epoch 00001: saving model to pesos_12a12_v2\\01-4.9219\n",
      "Epoch 2/20\n",
      "5000/5000 [==============================] - 493s 99ms/step - loss: 2.7294 - mae: 2.7294 - mape: 1327363.0000 - rmse: 5.2119 - val_loss: 4.5008 - val_mae: 4.5008 - val_mape: 10.6943 - val_rmse: 7.0598\n",
      "\n",
      "Epoch 00002: saving model to pesos_12a12_v2\\02-4.5008\n",
      "Epoch 3/20\n",
      "5000/5000 [==============================] - 487s 97ms/step - loss: 2.6220 - mae: 2.6220 - mape: 1285579.0000 - rmse: 5.0366 - val_loss: 5.1369 - val_mae: 5.1369 - val_mape: 13.1694 - val_rmse: 8.7508\n",
      "\n",
      "Epoch 00003: saving model to pesos_12a12_v2\\03-5.1369\n",
      "Epoch 4/20\n",
      "5000/5000 [==============================] - 489s 98ms/step - loss: 2.4989 - mae: 2.4989 - mape: 705747.0625 - rmse: 4.8911 - val_loss: 4.6585 - val_mae: 4.6585 - val_mape: 11.4593 - val_rmse: 7.6668\n",
      "\n",
      "Epoch 00004: saving model to pesos_12a12_v2\\04-4.6585\n",
      "Epoch 5/20\n",
      "5000/5000 [==============================] - 487s 97ms/step - loss: 2.4436 - mae: 2.4436 - mape: 2526790.5000 - rmse: 4.8452 - val_loss: 4.8385 - val_mae: 4.8385 - val_mape: 11.4439 - val_rmse: 7.4572\n",
      "\n",
      "Epoch 00005: saving model to pesos_12a12_v2\\05-4.8385\n",
      "Epoch 6/20\n",
      "5000/5000 [==============================] - 488s 98ms/step - loss: 2.3948 - mae: 2.3948 - mape: 2672428.2500 - rmse: 4.7754 - val_loss: 4.4195 - val_mae: 4.4195 - val_mape: 11.7595 - val_rmse: 8.0994\n",
      "\n",
      "Epoch 00006: saving model to pesos_12a12_v2\\06-4.4195\n",
      "Epoch 7/20\n",
      "5000/5000 [==============================] - 488s 98ms/step - loss: 2.3531 - mae: 2.3531 - mape: 2996509.2500 - rmse: 4.7211 - val_loss: 4.4544 - val_mae: 4.4544 - val_mape: 10.9671 - val_rmse: 7.2418\n",
      "\n",
      "Epoch 00007: saving model to pesos_12a12_v2\\07-4.4544\n",
      "Epoch 8/20\n",
      "5000/5000 [==============================] - 488s 98ms/step - loss: 2.3673 - mae: 2.3673 - mape: 2057743.1250 - rmse: 4.7822 - val_loss: 3.8141 - val_mae: 3.8141 - val_mape: 10.1113 - val_rmse: 7.1114\n",
      "\n",
      "Epoch 00008: saving model to pesos_12a12_v2\\08-3.8141\n",
      "Epoch 9/20\n",
      "5000/5000 [==============================] - 495s 99ms/step - loss: 2.3265 - mae: 2.3265 - mape: 5306800.0000 - rmse: 4.7245 - val_loss: 3.8171 - val_mae: 3.8171 - val_mape: 9.9135 - val_rmse: 6.9862\n",
      "\n",
      "Epoch 00009: saving model to pesos_12a12_v2\\09-3.8171\n",
      "Epoch 10/20\n",
      "5000/5000 [==============================] - 498s 100ms/step - loss: 2.3307 - mae: 2.3307 - mape: 580053.8750 - rmse: 4.6994 - val_loss: 3.8470 - val_mae: 3.8470 - val_mape: 10.1256 - val_rmse: 7.0713\n",
      "\n",
      "Epoch 00010: saving model to pesos_12a12_v2\\10-3.8470\n",
      "Epoch 11/20\n",
      "5000/5000 [==============================] - 495s 99ms/step - loss: 2.3026 - mae: 2.3026 - mape: 1327630.5000 - rmse: 4.7081 - val_loss: 3.8873 - val_mae: 3.8873 - val_mape: 10.6756 - val_rmse: 7.5228\n",
      "\n",
      "Epoch 00011: saving model to pesos_12a12_v2\\11-3.8873\n",
      "Epoch 12/20\n",
      "5000/5000 [==============================] - 493s 99ms/step - loss: 2.3059 - mae: 2.3059 - mape: 4049114.2500 - rmse: 4.7268 - val_loss: 3.8931 - val_mae: 3.8931 - val_mape: 10.2738 - val_rmse: 7.1628\n",
      "\n",
      "Epoch 00012: saving model to pesos_12a12_v2\\12-3.8931\n",
      "Epoch 13/20\n",
      "5000/5000 [==============================] - 522s 104ms/step - loss: 2.3118 - mae: 2.3118 - mape: 2793656.7500 - rmse: 4.7229 - val_loss: 4.1226 - val_mae: 4.1226 - val_mape: 10.6757 - val_rmse: 7.2826\n",
      "\n",
      "Epoch 00013: saving model to pesos_12a12_v2\\13-4.1226\n",
      "Epoch 14/20\n",
      "5000/5000 [==============================] - 516s 103ms/step - loss: 2.2823 - mae: 2.2823 - mape: 1330508.1250 - rmse: 4.6744 - val_loss: 3.8115 - val_mae: 3.8115 - val_mape: 10.1808 - val_rmse: 7.1325\n",
      "\n",
      "Epoch 00014: saving model to pesos_12a12_v2\\14-3.8115\n",
      "Epoch 15/20\n",
      "5000/5000 [==============================] - 513s 103ms/step - loss: 2.2436 - mae: 2.2436 - mape: 1755566.7500 - rmse: 4.6041 - val_loss: 3.6461 - val_mae: 3.6461 - val_mape: 9.8138 - val_rmse: 6.9101\n",
      "\n",
      "Epoch 00015: saving model to pesos_12a12_v2\\15-3.6461\n",
      "Epoch 16/20\n",
      "5000/5000 [==============================] - 508s 102ms/step - loss: 2.2316 - mae: 2.2316 - mape: 1731492.7500 - rmse: 4.6088 - val_loss: 3.2691 - val_mae: 3.2691 - val_mape: 8.6915 - val_rmse: 6.2860\n",
      "\n",
      "Epoch 00016: saving model to pesos_12a12_v2\\16-3.2691\n",
      "Epoch 17/20\n",
      "5000/5000 [==============================] - 509s 102ms/step - loss: 2.2552 - mae: 2.2552 - mape: 1893143.2500 - rmse: 4.6469 - val_loss: 3.2817 - val_mae: 3.2817 - val_mape: 8.7436 - val_rmse: 6.3494\n",
      "\n",
      "Epoch 00017: saving model to pesos_12a12_v2\\17-3.2817\n",
      "Epoch 18/20\n",
      "5000/5000 [==============================] - 516s 103ms/step - loss: 2.1925 - mae: 2.1925 - mape: 2937219.0000 - rmse: 4.5513 - val_loss: 4.0472 - val_mae: 4.0472 - val_mape: 11.1023 - val_rmse: 7.7690\n",
      "\n",
      "Epoch 00018: saving model to pesos_12a12_v2\\18-4.0472\n",
      "Epoch 19/20\n",
      "5000/5000 [==============================] - 516s 103ms/step - loss: 2.2147 - mae: 2.2147 - mape: 2260708.2500 - rmse: 4.5972 - val_loss: 3.2751 - val_mae: 3.2751 - val_mape: 9.1756 - val_rmse: 6.7154\n",
      "\n",
      "Epoch 00019: saving model to pesos_12a12_v2\\19-3.2751\n",
      "Epoch 20/20\n",
      "5000/5000 [==============================] - 511s 102ms/step - loss: 2.1950 - mae: 2.1950 - mape: 1205053.0000 - rmse: 4.5620 - val_loss: 4.7768 - val_mae: 4.7767 - val_mape: 12.7386 - val_rmse: 8.7008\n",
      "\n",
      "Epoch 00020: saving model to pesos_12a12_v2\\20-4.7768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21286fb2220>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = data.repeat(5)\n",
    "\n",
    "\n",
    "# train=data.take(200000)\n",
    "\n",
    "\n",
    "es_callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0.001, patience=5, restore_best_weights=True)\n",
    "\n",
    "tensorboard =  tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
    "\n",
    "\n",
    "\n",
    "model_Full.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "        loss=tf.keras.losses.MeanAbsoluteError(),\n",
    "        metrics=[tf.keras.metrics.MeanAbsoluteError(name=\"mae\"),tf.keras.metrics.MeanAbsolutePercentageError(name=\"mape\"),tf.keras.metrics.RootMeanSquaredError(name=\"rmse\")])\n",
    "# modelckpt_callback = keras.callbacks.ModelCheckpoint(monitor=\"val_loss\",filepath=\"sub_wheights_\"+str(rate)+\".hdf5\",verbose=1,save_weights_only=True,save_best_only=True)\n",
    "filepath = os.path.join(\"pesos_12a12_v2\", \"{epoch:02d}-{val_loss:.4f}\")\n",
    "modelckpt_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "filepath=filepath,\n",
    "verbose=1,\n",
    "mode=\"min\",\n",
    "monitor='val_loss',\n",
    "save_best_only=False,\n",
    "save_weights_only=True,\n",
    "save_freq='epoch')\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', mode='min', factor=0.5,\n",
    "                                                patience=10, verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "model_Full.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    batch_size=1,\n",
    "    epochs = 20, \n",
    "    steps_per_epoch=5000,\n",
    "    validation_data=(x_test,y_test),\n",
    "    validation_steps=4000,\n",
    "    use_multiprocessing=True,\n",
    "    callbacks =[modelckpt_callback,tensorboard,reduce_lr]\n",
    "    )\n",
    "\n",
    "# Recrear la taula d'experiments, direcament fer-ho a lo complert\n",
    "# !!!!!!!!!!!!!!!!!!!!Fes la puta acumulació de la taula!!!!!!!!!!!!!!!!!\n",
    "# Alguna cosa no va prou bé aqui, no es un problema que la sortida sigui un 352,12 i els de de validacio sirin 12, 352?\n",
    "# He intentat flipejar la sortida fent un reshape i mesta petant bastant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Full.save_weights(\"./pesos_bastant_entrenats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Full.load_weights(\"pesos_TRanformed_modificats_a12/18-1.1050\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[65.0531   65.02501  65.338776 64.93062  64.983284 65.18354  65.319336\n",
      " 65.23681  64.95065  64.887695 65.268585 65.190926 64.60709  64.53182\n",
      " 64.581345 64.9641   64.593155 64.881905 65.082405 64.96498  65.02519\n",
      " 65.1099   65.34745  65.511314 64.37474  64.27281  64.146935 65.04835\n",
      " 64.39915  64.7403   64.98682  64.850685 65.13344  65.30791  65.450874\n",
      " 65.76963  64.725914 64.6608   64.74747  65.0159   64.70739  64.975624\n",
      " 65.17336  65.06222  65.0718   65.123055 65.38756  65.51209  64.45816\n",
      " 64.33806  64.14024  65.25042  64.49821  64.84479  65.101555 64.96253\n",
      " 65.348045 65.54365  65.65149  66.026474 64.55637  64.48367  64.54231\n",
      " 64.90622  64.547966 64.834    65.03849  64.91592  64.973816 65.053055\n",
      " 65.3026   65.46352  64.82183  64.741615 64.757744 65.22194  64.81103\n",
      " 65.090614 65.28702  65.18443  65.27668  65.360504 65.573395 65.75393\n",
      " 64.46131  64.37153  64.33675  64.97182  64.46739  64.78233  65.006226\n",
      " 64.87582  65.04979  65.17653  65.37312  65.61319  64.3685   64.2129\n",
      " 63.76269  65.58932  64.445885 64.861755 65.16609  65.0241   65.696175\n",
      " 66.01965  65.96825  66.54971  64.48952  64.40799  64.38987  64.976295\n",
      " 64.49643  64.805305 65.02864  64.9033   65.047935 65.16912  65.373024\n",
      " 65.599174 65.568756 65.55923  65.998795 65.181335 65.45436  65.59813\n",
      " 65.67373  65.630745 65.16228  65.02143  65.44398  65.23499  64.74566\n",
      " 64.63965  64.444336 65.514435 64.77417  65.11086  65.348595 65.24489\n",
      " 65.57615  65.77412  65.853    66.211525 63.91388  63.785744 63.453\n",
      " 64.97405  63.983112 64.40567  64.715385 64.55009  65.083954 65.38725\n",
      " 65.41567  65.92514  64.18409  64.070206 63.87947  64.98315  64.22932\n",
      " 64.59474  64.86357  64.710304 65.0892   65.3008   65.419044 65.80167\n",
      " 64.941055 64.85016  64.78684  65.47794  64.9516   65.2352   65.44996\n",
      " 65.35087  65.53986  65.64589  65.82069  66.06038  64.53789  64.44737\n",
      " 64.36984  65.117836 64.54796  64.87092  65.10771  64.982834 65.188484\n",
      " 65.33318  65.50229  65.76801  64.38232  64.28664  64.13852  65.111\n",
      " 64.42721  64.766464 65.021835 64.8903   65.19678  65.38318  65.515144\n",
      " 65.85843  64.2464   64.14017  63.945103 65.05609  64.29434  64.65725\n",
      " 64.93083  64.785416 65.15183  65.36494  65.47709  65.85893  64.778175\n",
      " 64.71194  64.775085 65.11033  64.76824  65.036736 65.234924 65.130165\n",
      " 65.16425  65.22745  65.47395  65.618576 64.725136 64.64876  64.68203\n",
      " 65.112274 64.72269  64.99987  65.20632  65.09247  65.177536 65.25337\n",
      " 65.49099  65.66374  64.2818   64.1656   63.95959  65.10209  64.32727\n",
      " 64.68801  64.95801  64.81072  65.20324  65.41229  65.51962  65.90874\n",
      " 64.63987  64.55065  64.47253  65.22883  64.66569  64.97046  65.206375\n",
      " 65.0869   65.30614  65.436295 65.61406  65.88238  64.64413  64.57723\n",
      " 64.62088  65.0236   64.644035 64.92432  65.13426  65.02156  65.084045\n",
      " 65.16331  65.40327  65.57249  64.19322  64.09903  64.01175  64.814804\n",
      " 64.21556  64.560265 64.80696  64.66098  64.90359  65.07225  65.24146\n",
      " 65.53939  64.37216  64.295    64.33714  64.755905 64.36489  64.66884\n",
      " 64.87633  64.74495  64.82842  64.92828  65.164986 65.34936  65.006165\n",
      " 64.9333   64.94399  65.3822   64.97727  65.2549   65.4366   65.34935\n",
      " 65.412445 65.4998   65.681114 65.84694  63.77041  63.61494  63.1374\n",
      " 65.071434 63.863693 64.32578  64.66165  64.482185 65.205025 65.57307\n",
      " 65.51995  66.14969  64.612114 71.       67.2      70.3      66.8\n",
      " 68.3      66.3      66.7      67.7      65.9      68.1      67.9\n",
      " 67.       67.5      68.8      67.1      67.5      67.7      67.7\n",
      " 68.4      68.8      67.9      67.5      68.6      66.8      68.\n",
      " 68.4      68.4      68.6      66.1      68.       67.3      68.9\n",
      " 69.1      69.2      67.7      67.5      70.7      67.3      67.8\n",
      " 69.9      67.7      68.3      65.       66.       66.4      67.3\n",
      " 67.       68.8      67.4      67.       67.7      68.7      67.9\n",
      " 67.8      67.8      68.8      72.2      67.7      67.6      67.4\n",
      " 67.8      67.8      68.       67.8      66.       67.3      67.5\n",
      " 67.7      68.       67.4      66.7      65.       68.2      67.\n",
      " 67.5      67.3      69.1      67.5      68.1      69.5      67.7\n",
      " 67.5      67.8      67.5      68.8      66.5      67.4      67.4\n",
      " 67.5      67.5      67.1      68.4      68.7      66.9      68.7\n",
      " 67.4      66.7      68.3      65.5      70.7      67.5      68.7\n",
      " 70.1      68.3      67.7      68.1      66.6      72.1      68.9\n",
      " 72.2      66.4      68.3      67.2      66.9      67.4      69.3\n",
      " 67.5      67.4      68.3      67.       66.8      71.7      68.8\n",
      " 67.5      67.       67.5      69.1      69.       67.       68.2\n",
      " 66.7      65.9      66.7      66.3      66.5      67.1      65.3\n",
      " 66.8      66.4      67.4      67.4      70.4      71.3      67.6\n",
      " 67.       68.4      67.5      68.5      67.4      67.       67.4\n",
      " 70.3      67.9      69.1      68.8      68.8      69.7      68.6\n",
      " 67.8      66.2      68.3      67.4      67.9      65.9      67.8\n",
      " 68.       70.1      69.8      69.       70.6      67.9      68.2\n",
      " 70.1      68.2      70.4      70.3      70.3      67.7      69.7\n",
      " 68.7      65.8      68.6      66.8      70.2      66.9      67.5\n",
      " 67.6      68.3      69.3      68.8      68.9      67.3      66.7\n",
      " 71.2      66.8      68.       69.6      66.7      68.       69.\n",
      " 68.7      67.5      68.5      68.4      68.3      68.3      68.7\n",
      " 69.6      69.       68.2      67.6      68.3      69.       68.3\n",
      " 68.3      68.3      68.3      71.7      67.6      68.6      68.3\n",
      " 68.3      68.3      68.3      68.3      68.3      66.9      67.3\n",
      " 67.8      67.3      70.1      68.3      70.7      71.2      66.6\n",
      " 66.5      66.2      68.6      68.3      68.3      68.3      67.4\n",
      " 68.5      65.9      66.4      67.7      68.3      68.3      68.6\n",
      " 68.3      67.4      69.9      68.3      72.1      69.2      68.5\n",
      " 68.5      70.8      68.2      68.2      68.8      68.9      68.5\n",
      " 68.7      68.3      68.6      68.3      68.3      69.6      67.9\n",
      " 67.1      68.3      67.6      68.3      66.2      66.2      67.7\n",
      " 67.4      68.1      65.5      67.2      67.5      66.9      69.5\n",
      " 67.3      68.3      67.3      67.4      68.       67.       68.\n",
      " 64.9      59.3      65.3      67.3      68.       66.7      67.2\n",
      " 67.8      67.7      68.2      64.7      66.4      68.1      67.7\n",
      " 67.3      68.3      66.1      62.9      69.3      64.5      71.2\n",
      " 68.3      67.7      67.9      68.3      67.6      68.3      67.2\n",
      " 69.6      68.5      70.       68.3      71.1      67.9     ]\n"
     ]
    }
   ],
   "source": [
    "cpmar = np.concatenate([model_Full.predict(x_train[:1]),\n",
    "# tf.reshape(alll[12:24],[alll[12:24].shape[1],alll[12:24].shape[0]])]\n",
    "alll[12:24]],\n",
    "axis=1)\n",
    "print(cpmar[0])\n",
    "\n",
    "\n",
    "# plot.plot(cpmar,label=[\"pred\",\"real\"]) #Per un moment concret es el que tenim\n",
    "# plot.legend()\n",
    "# plot.xlabel(\"Nº sensors\")\n",
    "# plot.ylabel(\"Q metric\")\n",
    "# plot.show()\n",
    "# plot de coses com aquestes, queda maco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valors_sueltos = []\n",
    "\n",
    "for x in range(100):\n",
    "\n",
    "    cpmar = np.concatenate([model_Full.predict(alll[0+x:12+x]),np.expand_dims(alll[x+12],axis=1)],axis=1)\n",
    "    valors_sueltos.append(cpmar[135])\n",
    "\n",
    "plot.title(\"100 timesteps, sensor 135\")\n",
    "plot.plot(valors_sueltos,label=[\"pred\",\"real\"]) #Per un moment concret es el que tenim\n",
    "plot.legend()\n",
    "plot.xlabel(\"timestep\")\n",
    "plot.ylabel(\"Q metric\")\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pas_mig = model_F(hidden_F)\n",
    "pas_segon = model_F(hidden_F_next)\n",
    "#els nputs d'una gru son els seguents [batch, timesteps, feature]\n",
    "#la seguent activacio es una simple prova de funcionament\n",
    "inputs = tf.random.normal([22, 12, 6])\n",
    "\n",
    "le_gru = tf.keras.layers.GRU(1)\n",
    "# per cada gru pasar el valor de cada node per separat, despress reajuntar\n",
    "# en la primera podem considerar estat 0 pre gcn, features pures del node, i el estat 1 post gcn \n",
    "le_cell = tf.keras.layers.GRUCell(1)\n",
    "\n",
    "s_1, s_2 = le_cell(pas_mig, hidden_F)\n",
    "\n",
    "b_1, b_2 = le_cell(pas_segon, s_2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f37db822e937f17c1d05839aabf65484e8401eb0e88f5a7b956dce8c87ca9aa5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
